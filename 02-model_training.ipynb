{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ae4ec38-5a70-4288-941a-7f7b7f712659",
   "metadata": {},
   "source": [
    "# Model training with YOLOv5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e58114-57d5-4731-aa88-7e04ecd7f505",
   "metadata": {},
   "source": [
    "## Prepare YOLO\n",
    "\n",
    "This step is pretty easy, we only have to download YOLOv5 from Ultralytics and install the necessary dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09241b6-8f13-4e20-bfd2-74afee1170b4",
   "metadata": {},
   "source": [
    "### Download YOLOv5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68197c4e-f6c3-48d0-b066-5726e6a055e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'yolov5'...\n",
      "remote: Enumerating objects: 15352, done.\u001b[K\n",
      "remote: Total 15352 (delta 0), reused 0 (delta 0), pack-reused 15352\u001b[K\n",
      "Receiving objects: 100% (15352/15352), 14.33 MiB | 34.85 MiB/s, done.\n",
      "Resolving deltas: 100% (10497/10497), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ultralytics/yolov5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e89c6eb-98c7-4e9d-8756-e85a18af21c0",
   "metadata": {},
   "source": [
    "### We must slightly edit the requirements to have a container-compatible version of openCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07282081-61c0-499a-959d-710b477e9415",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -i 's/opencv-python/opencv-python-headless/' yolov5/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164e215b-7473-45a4-a91a-97b335546320",
   "metadata": {},
   "source": [
    "### Then install the requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8123c9c-80a7-4831-9bc7-b67a309fcd33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -qr yolov5/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514d72a7-04eb-4f22-b618-84d18f3e05d5",
   "metadata": {},
   "source": [
    "### Unfortunately, some subdependencies may have messed up with OpenCV. So let's make sure again we have the right version again..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b358c19d-c4d2-4562-bfdc-5884d437d46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping opencv-python as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -qy opencv-python\n",
    "!pip uninstall -qy opencv-python-headless\n",
    "!pip install -q opencv-python-headless"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3a490a-ad68-4bc5-afc5-cfdd048a93e9",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "668b83eb-5377-4e1e-b055-5914e55a9b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-03-28 15:55:51--  https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5n.pt\n",
      "Resolving github.com (github.com)... 140.82.114.4\n",
      "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/264818686/9e501477-46e9-4b14-97d9-0ef1ad7b3f3f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230328%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230328T155552Z&X-Amz-Expires=300&X-Amz-Signature=2ac4e70590ebf8251b251ead027ce9c48a3461c78a1f65b3b4adc3a7d51cbb79&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=264818686&response-content-disposition=attachment%3B%20filename%3Dyolov5n.pt&response-content-type=application%2Foctet-stream [following]\n",
      "--2023-03-28 15:55:52--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/264818686/9e501477-46e9-4b14-97d9-0ef1ad7b3f3f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230328%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230328T155552Z&X-Amz-Expires=300&X-Amz-Signature=2ac4e70590ebf8251b251ead027ce9c48a3461c78a1f65b3b4adc3a7d51cbb79&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=264818686&response-content-disposition=attachment%3B%20filename%3Dyolov5n.pt&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4062133 (3.9M) [application/octet-stream]\n",
      "Saving to: â€˜yolov5n.ptâ€™\n",
      "\n",
      "yolov5n.pt          100%[===================>]   3.87M  --.-KB/s    in 0.01s   \n",
      "\n",
      "2023-03-28 15:55:52 (287 MB/s) - â€˜yolov5n.ptâ€™ saved [4062133/4062133]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5n.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae012df-92f7-4fc0-b012-2159ccdd2139",
   "metadata": {},
   "source": [
    "### Create the configuration file\n",
    "\n",
    "There is a `configuration.yaml` file already present in the folder. Verify that it has the right number and names of the classes you want to use and that you downloaded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446ddbf6-0e6e-49c0-ba40-eb9108beabd3",
   "metadata": {},
   "source": [
    "### Freeze the YOLOv5 Backbone\n",
    "The backbone means the layers that extract input image features. We will freeze the backbone so the weights in the backbone layers will not change during YOLOv5 transfer learning. We will then only train the last layers (the head layers).\n",
    "\n",
    "In this example, we will use the smallest available base model, yolov5n. If we open the file `yolov5/models/yolov5n.yaml` we can see the following structure:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9865eca3-13ab-4c93-bf49-d08442a58925",
   "metadata": {},
   "source": [
    "# YOLOv5 ðŸš€ by Ultralytics, GPL-3.0 license\n",
    "\n",
    "# Parameters\n",
    "nc: 80  # number of classes\n",
    "depth_multiple: 0.33  # model depth multiple\n",
    "width_multiple: 0.25  # layer channel multiple\n",
    "anchors:\n",
    "  - [10,13, 16,30, 33,23]  # P3/8\n",
    "  - [30,61, 62,45, 59,119]  # P4/16\n",
    "  - [116,90, 156,198, 373,326]  # P5/32\n",
    "\n",
    "# YOLOv5 v6.0 backbone\n",
    "backbone:\n",
    "  # [from, number, module, args]\n",
    "  [[-1, 1, Conv, [64, 6, 2, 2]],  # 0-P1/2\n",
    "   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4\n",
    "   [-1, 3, C3, [128]],\n",
    "   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8\n",
    "   [-1, 6, C3, [256]],\n",
    "   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16\n",
    "   [-1, 9, C3, [512]],\n",
    "   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32\n",
    "   [-1, 3, C3, [1024]],\n",
    "   [-1, 1, SPPF, [1024, 5]],  # 9\n",
    "  ]\n",
    "\n",
    "# YOLOv5 v6.0 head\n",
    "head:\n",
    "  [[-1, 1, Conv, [512, 1, 1]],\n",
    "   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n",
    "   [[-1, 6], 1, Concat, [1]],  # cat backbone P4\n",
    "   [-1, 3, C3, [512, False]],  # 13\n",
    "\n",
    "   [-1, 1, Conv, [256, 1, 1]],\n",
    "   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n",
    "   [[-1, 4], 1, Concat, [1]],  # cat backbone P3\n",
    "   [-1, 3, C3, [256, False]],  # 17 (P3/8-small)\n",
    "\n",
    "   [-1, 1, Conv, [256, 3, 2]],\n",
    "   [[-1, 14], 1, Concat, [1]],  # cat head P4\n",
    "   [-1, 3, C3, [512, False]],  # 20 (P4/16-medium)\n",
    "\n",
    "   [-1, 1, Conv, [512, 3, 2]],\n",
    "   [[-1, 10], 1, Concat, [1]],  # cat head P5\n",
    "   [-1, 3, C3, [1024, False]],  # 23 (P5/32-large)\n",
    "\n",
    "   [[17, 20, 23], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)\n",
    "  ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba8e6fd-ac8a-4021-9827-b189837b9e89",
   "metadata": {},
   "source": [
    "In the **backbone** section we can see that there are 10 layers. So that's the number of layers we want to freeze in our training!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bcfae3-e88f-42e9-9c69-7502f5f97acf",
   "metadata": {},
   "source": [
    "### Let's set some training parameters (hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "390782e8-5506-456c-91de-1aec83cafd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = '50' # The number of trainingiterations.\n",
    "# This is voluntarily low to save processing time for this example. Adjust for better results (>150).\n",
    "\n",
    "batch = '64' # The number of images analyzed in a single pass\n",
    "# You may have to adjust this depending on the memory available on your GPU.\n",
    "\n",
    "model_weights = 'yolov5n.pt' # Smallest available model to save processing time in this example. Adjust for better results\n",
    "# Other available models: https://github.com/ultralytics/yolov5#pretrained-checkpoints\n",
    "\n",
    "freeze_layers = '10' # As per above, we want to freeze 10 layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2231bb5-081a-4e98-872b-89073a80ef18",
   "metadata": {},
   "source": [
    "### We can now launch the training!\n",
    "\n",
    "NOTE: PyTorch first caches images to speed up the process. If you have enough memory and shared memory that is not an issue. However, this may not be always the case, especially with large datasets. Therefore the cache is forced to disk here. But of course this is a parameter you can change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7463168-e2e1-429d-92e4-86486059afa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "NameError: name 'yolov5n' is not defined\n"
     ]
    }
   ],
   "source": [
    "!python -c \"\"\"print({model_weights})\"\"\"\n",
    "#!python yolov5/train.py --data configuration.yaml --weights ${model_weights} --epochs ${epochs} --batch ${batch} --freeze ${freeze} --cache disk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.14",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
